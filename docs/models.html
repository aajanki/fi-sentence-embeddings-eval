<!doctype html>
<!--
Template created by Andrew G. York, with modifications by Antti
Ajanki, based on this theme by Diana Mounter:
https://github.com/broccolini/dinky, which mentioned that attribution
is appreciated. Thanks, broccolini!
-->
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
  <title>Sentence embedding models</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="templates/styles/scholar-multipage.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="wrapper">
<article typeof="ScholarlyArticle" vocab="http://schema.org/">
    <!-- <header class="article-header"> -->
<h1 property="headline">Sentence embedding models</h1>
<!-- </header> -->
<p class="author-list">
</p>
<div class="author_affiliations">
</div>
<div class="author-info">
        </div>





<div property="articleBody" class="article-body">
<p>This page introduces the sentence embedding models under study and the evaluation architecture. The <a href="results.html">results of the study</a> are presented on a separate page.</p>
<h2 id="introduction-to-sentence-embedding">Introduction to sentence embedding</h2>
<p>A crucial component in most natural language processing (NLP) applications is finding an expressive representation for text. Modern methods are typically based on sentence embeddings that map a sentence onto a numerical vector. The vector attempts to capture the semantic content of the text. If two sentences express a similar idea using different words, their representations (embedding vectors) should still be similar to each other.</p>
<p>Several methods for constructing these embeddings have been proposed in the literature. Interestingly, learned embeddings tend generalize quite well to other material and NLP tasks besides the ones they were trained on. This is fortunate, because it allows us to use pre-trained models and avoid expensive training. (A single training run on a modern, large-scale NLP model can cost <a href="https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/">up to tens of thousands of dollars</a>).</p>
<p>Sentence embeddings are being applied on almost all NLP application areas. In information retrieval they are used for comparing the meanings of text snippets, machine translation uses sentence embeddings as an “intermediate language” when translating between two human languages and many classification and tagging applications are based on embeddings. With better representations it is possible to build applications that react more naturally to the sentiment and topic of written text.</p>
<h2 id="evaluated-sentence-embedding-models">Evaluated sentence embedding models</h2>
<p>This study compares sentence embedding models which have a pre-trained Finnish variant publicly available. State-of-the-art models without a published Finnish variant, such as GPT-2 and XLNet, are not included in the evaluation.</p>
<p>The most important sentence embedding models can be divided into three categories: baseline bag-of-word models, models that are based on aggregating embeddings for the individual words in a sentence, and models that build a representation for a full sentence.</p>
<h3 id="bag-of-words-bow">Bag-of-words (BoW)</h3>
<p><strong>TF-IDF</strong>. I’m using <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> (term frequency-inverse document frequency) vectors as a baseline. A TF-IDF vector is a sparse vector with one dimension per each unique word in the vocabulary. The value for an element in a vector is an occurrence count of the corresponding word in the sentence multiplied by a factor that is inversely proportional to the overall frequency of that word in the whole corpus. The latter factor is meant to diminish the effect of very common words (<em>mutta</em>, <em>ja</em>, <em>siksi</em>, …) which are unlikely to tell much about the actual content of the sentence. The vectors are L2-normalized to reduce the effect of differing sentence lengths.</p>
<p>As a preprocessing, words are converted to their dictionary form (lemmatized) and all unigrams and bigrams occurring more than four times are selected. Typically (depending on the corpus) this results in dimensionality (i.e. the vocabulary size) around 5000. TF-IDF vectors are very sparse, because one document usually contains only a small subset of all words in the vocabulary.</p>
<p>BoW models treat each word as an equivalent entity. They don’t consider the semantic meaning of words. For example, a BoW model doesn’t understand that a <em>horse</em> and a <em>pony</em> are more similar than a <em>horse</em> and a <em>monocle</em>. Next, we’ll move on to word embedding models that have been introduced to take the semantic information better into account.</p>
<h3 id="pooled-word-embeddings">Pooled word embeddings</h3>
<p>A <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> is a vector representation of a word. Words, which often occur in similar context (a <em>horse</em> and a <em>pony</em>), are assigned vector that are close by each other and words, which rarely occur in a similar context (a <em>horse</em> and a <em>monocle</em>), dissimilar vectors. The embedding vectors are dense, relatively low dimensional (typically 50-300 dimensions) vectors.</p>
<p>The embedding vectors are typically trained on a language modeling task: predict the presentation for a word given the representation of a few previous (and possibly following) words. This is a self-supervised task as the supervision signal is the order of the word in a document. The training requires just a large corpus of text documents. It is well established that word embeddings learned on a language modeling task generalize well to other downstream tasks: classification, part-of-speech tagging and so on.</p>
<p><strong>Pooled word2vec</strong>. One of the earliest and still widely used word embedding model is the word2vec model <span class="citation" data-cites="mikolov2013">(Mikolov, Chen, Corrado, &amp; Dean, 2013)</span>. I’m using the Finnish word2vec vectors trained on the <a href="https://turkunlp.org/finnish_nlp.html#parsebank">Finnish Internet Parsebank</a> data by the <a href="https://turkunlp.org/">Turku NLP research group</a>. The embeddings are 300 dimensional.</p>
<p>There are several ways to get from the word embeddings to sentence embeddings. Perhaps, the most straight-forward idea is to aggregate the embeddings of each word that appears in a sentence, for example, by taking an element-wise average, minimum or maximum. These strategies are called average-pooling, min-pooling and max-pooling, respectively. Still another alternative is the concatenation of min- and max-pooled vectors. In this work, I’m evaluating average-pooled word2vec. Average-pooling performed better than the alternatives on a brief preliminary study on the TDT dataset. As this might be dataset-dependent, it’s a good idea to experiment with different pooling strategies.</p>
<p><strong>Pooled FastText</strong> <span class="citation" data-cites="bojanowski2017">(Bojanowski, Grave, Joulin, &amp; Mikolov, 2017)</span> extends the word2vec by generating embeddings for subword units (character n-grams). By utilizing the subword structure, FastText aims to provide better embeddings for rare and out-of-vocabulary words. It should help particularly on morpheme-rich languages like Finnish (Finnish uses lots of inflections). The authors of FastText have published a Finnish model trained on Finnish subsets of <a href="http://commoncrawl.org/">Common Crawl</a> and <a href="https://www.wikipedia.org/">Wikipedia</a>. I’m average-pooling the word embeddings to get an embedding for the sentence.</p>
<p>Researchers have proposed also other aggregation methods besides simple pooling. I’m evaluating two such proposals here: SIF and BOREP.</p>
<p><strong>SIF</strong>. <span class="citation" data-cites="arora2017">(Arora, Liang, &amp; Ma, 2017)</span> introduces a model called smooth inverse frequency (SIF). They propose taking a weighted average (weighted by a term related to the inverse document frequency) of the word embeddings in a sentence and then removing the projection of the first singular vector. This is derived from an assumption that the sentence has been generated by a random walk of a discourse vector on a latent word embedding space, and by including smoothing terms for frequent words.</p>
<p><strong>BOREP</strong>. The idea of BOREP or bag of random embedding projections <span class="citation" data-cites="wieting2019">(Wieting &amp; Kiela, 2019)</span> is to project word embeddings to a random high-dimensional space and pool the projections there. The intuition is that casting things into a higher dimensional space tends to make them more likely to be linearly separable thus making the text classification easier. In the evaluations, I’m projecting to a 4096 dimensional space following the lead of the paper authors.</p>
<p>While word embeddings capture some aspects of semantics, pooling embeddings is quite a crude way of aggregating the meaning of a sentence. Pooling for example completely ignores the order of the words. The current state-of-the-art in NLP is deep learning models that take a whole sentence as an input and are thus capable of processing the full context of the sentence. Let’s take a look at two such models.</p>
<h3 id="contextual-full-sentence-embeddings">Contextual full sentence embeddings</h3>
<p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="devlin2018">(Devlin, Chang, Lee, &amp; Toutanova, 2018)</span> processes a full sentence to generate a sentence embeddings. It uses self-attention to decide the relevant context for each word in the sentence. Another major technical contribution in the paper was a bi-directional transformer architecture, which allows the model to consider word’s left and right context when making predictions.</p>
<p>In the evaluation, I’ll use the value of the second-to-last hidden layer of a special [CLS] token as the final sentence embedding. BERT is trained to aggregate information about the whole sequence on the layers corresponding to the [CLS] token on classification tasks. I have selected the second-to-last hidden layer because both the paper and my brief preliminary study showed that it gives slightly better classification performance than the output layer of the [CLS] token. BERT also generates output embeddings for each input word, but these are not used in the evaluations. In the paraphrase and consecutive sentence evaluation tasks that directly compare two sentences, both sentences are fed as input separated by a separator token to match how BERT was trained.</p>
<p>I’m using the pre-trained multilingual (Bert-base, multilingual cased) variant published by the BERT authors. It has been trained on 104 languages, including Finnish. The embedding dimensionality is 768.</p>
<p><strong>LASER</strong>. As the second contextual sentence embedding method, I’ll evaluate LASER (Language-Agnostic SEntence Representations) by <span class="citation" data-cites="artetxe2018">(Artetxe &amp; Schwenk, 2018)</span>. It is specifically meant to learn language-agnostic sentence embeddings. It has a similar deep bi-directional architecture as BERT, but uses LSTM encoders instead of transformer blocks like BERT. I’m using the pre-trained model published by the LASER authors. It produces 1024 dimensional embedding vectors.</p>
<h2 id="sentence-classifier-architecture">Sentence classifier architecture</h2>
<p>The sentence embedding models are evaluated on sentence classification tasks (given a sentence output the class it belongs to) or sentence pair comparison tasks (given a pair of sentences output a binary yes/no judgment: are the two sentences paraphrases or do they belong to the same document). Sentence embedding models are combined with a task-specific classifier neural network.</p>
<p>The architecture used in the evaluations is show on the image below. The sentence embedding model under evaluation (the blue block) converts the sentence text into a sentence embedding vector which is the input for a task-specific classifier (the orange blocks). The classifier consists of a dense hidden layer with a sigmoid activation and a softmax output layer. Both layers are regularized by dropout. The dimensionality, weights and dropout probabilities of the classifier are optimized on the development dataset (separately for each task and embedding model) but the pre-trained sentence embedding model is keep fixed during the whole experiment.</p>
<figure>
<img src="images/sentence_classifier.png" alt="Architecture for a) sentence classification and b) sentence pair classification tasks. The blue blocks are the pre-trained sentence embedding models under study. Their parameters are kept fixed. The orange blocks form the classifier, which is optimized for each task. The green blocks are fixed vector operators." /><figcaption>Architecture for a) sentence classification and b) sentence pair classification tasks. The blue blocks are the pre-trained sentence embedding models under study. Their parameters are kept fixed. The orange blocks form the classifier, which is optimized for each task. The green blocks are fixed vector operators.</figcaption>
</figure>
<p>For sentence pair tasks (labeled as b) in the image), I’m using a technique introduced by <span class="citation" data-cites="tai2015">(Tai, Socher, &amp; Manning, 2015)</span>: first, sentence embeddings are generated for the two sentences separately, and then they are merged into a single feature vector that represents the pair (the green blocks). The merging is done as follows: Let the embeddings for the two sentences be called <span class="math inline"><em>u</em></span> and <span class="math inline"><em>v</em></span>. The feature vector for the pair is then generated as a concatenation of the element-wise product <span class="math inline"><em>u</em> ⊙ <em>v</em></span> and the element-wise absolute distance <span class="math inline">|<em>u</em> − <em>v</em>|</span>. The concatenated feature vector is then used as the input for the classification part, like above. The BERT model is an exception. It has an integrated way of handling sentence pair tasks (see above).</p>
<p>The final evaluation <a href="results.html">results</a> are computed on a test set that has not been used during the training.</p>
<p>The pre-trained sentence embedding models are treated as black box feature extractors that output embedding vectors. An alternative approach is to fine-tune a pre-trained embedding model by optimizing the full pipeline (usually with just a small learning rate for the embedding part). Feature extraction is computationally cheaper, but fine-tuning potentially adapts the embeddings better to various tasks. The relative performance of feature extraction and fine-tuning approaches depends on the similarity of the pre-training and target tasks <span class="citation" data-cites="peters2019">(Peters, Ruder, &amp; Smith, 2019)</span> and its effects should be studied on each task separately. The current evaluation focuses only on the feature extraction strategy to understand what can be achieved on relatively low computational resources.</p>
<h2 id="references">References</h2>

<p></p>
<div id="refs" class="references">
<div id="ref-arora2017">
<p>Arora, S., Liang, Y., &amp; Ma, T. (2017). A simple but tough-to-beat baseline for sentence embeddings. In <em>International conference on learning representations (ICLR 2017)</em>. Retrieved from <a href="https://openreview.net/forum?id=SyK00v5xx" class="uri">https://openreview.net/forum?id=SyK00v5xx</a></p>
</div>
<div id="ref-artetxe2018">
<p>Artetxe, M., &amp; Schwenk, H. (2018). Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. <em>arXiv Preprint arXiv:1812.10464</em>. Retrieved from <a href="https://arxiv.org/abs/1812.10464" class="uri">https://arxiv.org/abs/1812.10464</a></p>
</div>
<div id="ref-bojanowski2017">
<p>Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2017). Enriching word vectors with subword information. <em>Transactions of the Association for Computational Linguistics</em>, <em>5</em>, 135–146. Retrieved from <a href="https://arxiv.org/abs/1607.04606" class="uri">https://arxiv.org/abs/1607.04606</a></p>
</div>
<div id="ref-devlin2018">
<p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv Preprint arXiv:1810.04805</em>. Retrieved from <a href="https://arxiv.org/abs/1810.04805" class="uri">https://arxiv.org/abs/1810.04805</a></p>
</div>
<div id="ref-mikolov2013">
<p>Mikolov, T., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. In <em>International conference on learning representations (ICLR 2013)</em>. Retrieved from <a href="https://arxiv.org/abs/1301.3781" class="uri">https://arxiv.org/abs/1301.3781</a></p>
</div>
<div id="ref-peters2019">
<p>Peters, M. E., Ruder, S., &amp; Smith, N. A. (2019). To tune or not to tune? Adapting pretrained representations to diverse tasks. <em>arXiv Preprint arXiv:1903.05987</em>. Retrieved from <a href="https://arxiv.org/abs/1903.05987" class="uri">https://arxiv.org/abs/1903.05987</a></p>
</div>
<div id="ref-tai2015">
<p>Tai, K. S., Socher, R., &amp; Manning, C. D. (2015). Improved semantic representations from tree-structured long short-term memory networks. In <em>Association for computational linguistics (ACL 2015)</em>. Retrieved from <a href="https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf" class="uri">https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf</a></p>
</div>
<div id="ref-wieting2019">
<p>Wieting, J., &amp; Kiela, D. (2019). No training required: Exploring random encoders for sentence classification. <em>arXiv Preprint arXiv:1901.10444</em>. Retrieved from <a href="https://arxiv.org/abs/1901.10444" class="uri">https://arxiv.org/abs/1901.10444</a></p>
</div>
</div>
</div>
</article>
<footer>
  <div class="table-of-contents">
    <h3>Table of contents</h3>
    <ul>
      <li><a href="index.html">Results<a></li>
      <li><a href="models.html">Embedding models<a></li>
      <li><a href="tasks.html">Evaluation tasks<a></li>
    </ul>
  </div>
  <p><small>Generated using <a href="https://github.com/pandoc-scholar/pandoc-scholar">pandoc scholar</a></small></p>
</footer>
</div>
 <!--[if !IE]><script>fixScale(document);</script><![endif]-->
</body>
</html>
